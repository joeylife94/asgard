"""Ollama API í†µì‹  ëª¨ë“ˆ"""

import os
import time
import requests
from typing import Optional, Dict, Any, Iterator
from rich.console import Console


class OllamaClient:
    """Ollama API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(
        self,
        url: str = "http://localhost:11434",
        model: str = "mistral",
        timeout: int = 120,
        max_retries: int = 3,
    ):
        self.url = url.rstrip('/')
        self.model = model
        self.timeout = timeout
        self.max_retries = max_retries
        self.console = Console()
    
    def analyze(
        self,
        prompt: str,
        stream: bool = False,
    ) -> Dict[str, Any]:
        """
        ë¡œê·¸ ë¶„ì„ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        
        Returns:
            {"response": str, "metadata": dict}
        """
        for attempt in range(self.max_retries):
            try:
                if stream:
                    return self._analyze_stream(prompt)
                else:
                    return self._analyze_blocking(prompt)
            
            except requests.exceptions.ConnectionError:
                if attempt < self.max_retries - 1:
                    wait_time = 2 ** attempt  # exponential backoff
                    self.console.print(
                        f"[yellow]âš ï¸  ì—°ê²° ì‹¤íŒ¨, {wait_time}ì´ˆ í›„ ì¬ì‹œë„... ({attempt+1}/{self.max_retries})[/yellow]"
                    )
                    time.sleep(wait_time)
                else:
                    raise Exception(
                        f"Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({self.url})\n"
                        "Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: ollama serve"
                    )
            
            except requests.exceptions.Timeout:
                if attempt < self.max_retries - 1:
                    self.console.print(
                        f"[yellow]âš ï¸  íƒ€ì„ì•„ì›ƒ, ì¬ì‹œë„... ({attempt+1}/{self.max_retries})[/yellow]"
                    )
                else:
                    raise Exception(f"Ollama ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ ({self.timeout}ì´ˆ)")
            
            except Exception as e:
                # Ollama returns HTTP 404 when the requested model isn't available locally.
                # For E2E/CI environments we optionally allow a deterministic fallback so
                # the orchestration loop can be validated without downloading large models.
                if self._is_model_not_found(e) and self._allow_fallback():
                    return self._fallback_analysis(prompt, reason="ollama_model_not_available")
                raise Exception(f"Ollama API ìš”ì²­ ì‹¤íŒ¨: {e}")

    def _allow_fallback(self) -> bool:
        return os.getenv("BIFROST_OLLAMA_ALLOW_FALLBACK", "false").lower() in (
            "true",
            "1",
            "yes",
        )

    def _is_model_not_found(self, error: Exception) -> bool:
        if not isinstance(error, requests.exceptions.HTTPError):
            return False
        response = getattr(error, "response", None)
        if response is None:
            return False
        return response.status_code == 404

    def _fallback_analysis(self, prompt: str, reason: str) -> Dict[str, Any]:
        start_time = time.time()
        response = (
            "## ğŸ“Š ìš”ì•½\n"
            "Ollama ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•„(ë‹¤ìš´ë¡œë“œ/ë¡œë“œ í•„ìš”) ì„ì‹œ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤.\n\n"
            "## ğŸ” ì£¼ìš” ì´ìŠˆ\n"
            "- LLM ëª¨ë¸ì´ ë¡œì»¬ì— ì¡´ì¬í•˜ì§€ ì•Šì•„ `/api/generate` ìš”ì²­ì´ 404ë¡œ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n"
            "- Kafka ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ê²½ë¡œ(ìš”ì²­â†’ì²˜ë¦¬â†’ê²°ê³¼)ëŠ” ì •ìƒ ë™ì‘ ì¤‘ì…ë‹ˆë‹¤.\n\n"
            "## ğŸ’¡ ì œì•ˆì‚¬í•­\n"
            "- E2E í™˜ê²½ì—ì„œ ëª¨ë¸ì„ ë¯¸ë¦¬ ì¤€ë¹„í•˜ì„¸ìš”: `ollama pull <model>`\n"
            "- ë˜ëŠ” E2Eì—ì„œëŠ” `BIFROST_OLLAMA_ALLOW_FALLBACK=true` ìœ ì§€ í›„, í”„ë¡œë•ì…˜ì—ì„œëŠ” ë„ì„¸ìš”.\n"
        )
        duration = time.time() - start_time
        return {
            "response": response,
            "metadata": {
                "model": "fallback",
                "duration": round(duration, 2),
                "done": True,
                "fallback_reason": reason,
                "requested_model": self.model,
            },
        }
    
    def _analyze_blocking(self, prompt: str) -> Dict[str, Any]:
        """ë¸”ë¡œí‚¹ ëª¨ë“œ ë¶„ì„"""
        api_endpoint = f"{self.url}/api/generate"
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
        }
        
        start_time = time.time()
        response = requests.post(
            api_endpoint,
            json=payload,
            timeout=self.timeout,
        )
        response.raise_for_status()
        duration = time.time() - start_time
        
        result = response.json()
        
        return {
            "response": result.get("response", ""),
            "metadata": {
                "model": self.model,
                "duration": round(duration, 2),
                "done": result.get("done", False),
            }
        }
    
    def _analyze_stream(self, prompt: str) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ë¶„ì„"""
        api_endpoint = f"{self.url}/api/generate"
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": True,
        }
        
        start_time = time.time()
        response = requests.post(
            api_endpoint,
            json=payload,
            stream=True,
            timeout=self.timeout,
        )
        response.raise_for_status()
        
        # ìŠ¤íŠ¸ë¦¼ ìˆ˜ì§‘
        full_response = []
        for line in response.iter_lines():
            if line:
                import json
                chunk = json.loads(line)
                if text := chunk.get("response"):
                    full_response.append(text)
                    # ì‹¤ì‹œê°„ ì¶œë ¥
                    print(text, end='', flush=True)
        
        duration = time.time() - start_time
        print()  # ì¤„ë°”ê¿ˆ
        
        return {
            "response": ''.join(full_response),
            "metadata": {
                "model": self.model,
                "duration": round(duration, 2),
                "done": True,
            }
        }
    
    def health_check(self) -> bool:
        """Ollama ì„œë²„ í—¬ìŠ¤ ì²´í¬"""
        try:
            response = requests.get(f"{self.url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë ˆê±°ì‹œ í•¨ìˆ˜
def analyze_with_ollama(
    prompt: str,
    ollama_url: str = "http://localhost:11434",
    model: str = "mistral",
    stream: bool = False,
) -> str:
    """ë ˆê±°ì‹œ í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    client = OllamaClient(url=ollama_url, model=model)
    result = client.analyze(prompt, stream=stream)
    return result["response"]
